<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="transformer笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="http://example.com/2024/09/06/20240906_Transformer/index.html">
<meta property="og:site_name" content="个人记录">
<meta property="og:description" content="transformer笔记">
<meta property="og:locale">
<meta property="article:published_time" content="2024-09-06T10:41:58.000Z">
<meta property="article:modified_time" content="2024-11-22T16:32:30.260Z">
<meta property="article:author" content="JiuZhouu">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/2024/09/06/20240906_Transformer/"/>





  <title>Transformer | 个人记录</title>
  








<meta name="generator" content="Hexo 7.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">个人记录</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/09/06/20240906_Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/custom/jiuzhou.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人记录">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Transformer</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-09-06T18:41:58+08:00">
                2024-09-06
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2024-11-23T00:32:30+08:00">
                2024-11-23
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="url" rel="index">
                    <span itemprop="name">计算机</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/Transformer/" itemprop="url" rel="index">
                    <span itemprop="name">Transformer</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>transformer笔记</p>
<span id="more"></span>

<p>问：解释一下什么是RNN以及它在NLP中的应用。<br>答：循环神经网络（RNN）是一种用于处理序列数据的神经网络，适合于时间序列数据或自然语言文本。RNN在每个时间步骤上接受输入，并维护一个隐藏状态来捕捉之前时间步的信息。在NLP中，RNN被用于语言模型、文本生成、机器翻译等任务，能够处理变长的文本序列。</p>
<p>问：请解释LSTM和它与传统RNN的区别。<br>答：长短期记忆网络（LSTM）是RNN的一种变体，专门设计来解决RNN长期依赖问题。与传统RNN相比，LSTM通过引入三个门（输入门、遗忘门、输出门）和一个细胞状态来控制信息的流动，从而更有效地捕捉长期依赖关系。这使得LSTM在处理具有长距离依赖的NLP任务时表现更佳。</p>
<p>问：什么是Transformer模型，它为什么在NLP领域如此重要？<br>答：Transformer模型是一种基于自注意力机制的深度学习模型，能够在处理序列数据时并行化计算，从而显著提高效率。Transformer通过自注意力机制捕获序列内的全局依赖关系，无需像RNN那样顺序处理数据。它在NLP领域的重要性在于，Transformer模型是许多先进NLP模型（如BERT、GPT）的基础架构，这些模型在多种语言任务上取得了突破性的性能。</p>
<p>问：BERT模型是如何工作的？<br>答：BERT（Bidirectional Encoder Representations from Transformers）是一个基于Transformer的预训练模型，通过在大规模文本语料库上预训练，学习文本的深层次语言表示。BERT的独特之处在于它的双向训练，即同时考虑上下文中的左侧和右侧信息。这使得BERT能够更好地理解词汇在不同上下文中的含义，提升了模型在下游NLP任务（如情感分析、问答系统等）上的表现。</p>
<p>问：解释一下注意力机制及其重要性。<br>答：注意力机制是一种让模型能够在处理信息时“关注”到重要部分的技术。在NLP中，注意力机制允许模型在生成输出时，根据需要动态地聚焦于输入序列的不同部分。这种机制显著提高了模型处理长序列和理解复杂语言结构的能力，尤其在翻译、摘要等任务中至关重要。</p>
<p>问：什么是语言模型？<br>答：语言模型是用来预测序列中下一个词出现的概率的模型。它基于前面的词来预测序列中的下一个词，从而能够生成文本或者评估给定文本序列的概率。语言模型是许多NLP任务的基础，如文本生成、语音识别、拼写校正等。</p>
<p>问：解释一下多任务学习在NLP中的应用。<br>答：多任务学习是一种训练模型同时解决多个相关任务的方法。在NLP中，多任务学习可以通过共享表示学习更通用的语言特征，提高模型在特定任务上的性能。例如，同时进行语言识别和情感分析可以让模型更好地理解文本内容和情感。</p>
<p>问：什么是GPT？它是如何工作的？<br>答：GPT（Generative Pre-trained Transformer）是一个基于Transformer的预训练语言模型，设计用于文本生成。GPT通过在大量文本上预训练，学习文本的深层次表示，然后在特定任务上进行微调。GPT的工作机制是使用上下文中的前面的词来预测下一个词，从而生成连贯的文本序列。</p>
<p>问：解释零样本学习和它在NLP中的重要性。<br>答：零样本学习是一种让模型能够处理在训练过程中未见过的类别的技术。在NLP中，这意味着模型能够理解和生成之前未见过的话题或概念的文本。这对于构建通用的、适应性强的NLP系统非常重要，因为它允许模型在没有明确示例的情况下处理新任务和概念。</p>
<p>问：解释一下迁移学习在NLP中的应用。<br>答：迁移学习是一种机器学习技术，它允许模型将从一个任务学到的知识应用到另一个相关的任务上。在NLP中，通过在大量文本数据上预训练一个模型，然后在特定任务（如情感分析、文本分类等）上进行微调，可以显著提高模型的性能和泛化能力。</p>
<p>问：解释一下对抗训练在NLP中的作用。<br>答：对抗训练是一种通过向模型输入微小的、意图造成误差的扰动来增强模型鲁棒性的方法。在NLP中，这可以帮助模型抵抗诸如错别字或故意歪曲的文本等小的变化，提高模型在面对攻击或噪声数据时的稳定性和准确性。</p>
<p>问：什么是深度学习在NLP中的主要挑战？<br>答：深度学习在NLP中面临的主要挑战包括处理自然语言的高度复杂性和多样性、理解语境和隐含含义、处理长距离依赖问题以及需要大量标注数据进行训练。此外，解释模型的决策和保证模型的公平性、透明性也是重要挑战</p>
<p>问：解释自注意力机制的工作原理及其在Transformer模型中的作用。<br>答：自注意力机制允许输入序列中的每个位置计算其对序列中所有位置的注意力权重，并基于这些权重生成加权和表示。数学上，对于每个位置 i，自注意力输出是输入序列的加权和，权重由位置 i 和序列中其他位置的相对重要性决定。在Transformer模型中，自注意力机制使模型能够捕捉长距离依赖，提高了并行处理能力，极大地提升了模型处理复杂语言任务的能力。</p>
<p>问：BERT模型如何处理词汇之间的多种关系？<br>答：BERT模型通过预训练任务，如Masked Language Model（MLM）和Next Sentence Prediction（NSP），学习词汇之间的多种关系。MLM随机遮蔽输入序列中的词汇，并让模型预测这些被遮蔽的词，这迫使模型学习词汇间的上下文关系。NSP则让模型预测两个句子是否连续出现，促使模型理解句子间的逻辑关系。</p>
<p>问：解释Transformer中的位置编码的作用及其实现方式。<br>答：由于Transformer完全基于自注意力机制，它自身不像RNN或CNN那样具有处理序列数据的顺序感知能力。位置编码的作用是为模型提供关于单词在序列中位置的信息。一种常见的实现方式是使用正弦和余弦函数的组合来生成每个位置的位置编码，然后将这些编码加到输入嵌入上。这种方式可以保持相对位置信息，并允许模型利用序列的顺序信息。</p>
<p>问：什么是零次学习（Zero-shot Learning）在NLP中的应用？ 答：零次学习是指让模型在没有直接经过训练的特定任务上执行任务的能力。在NLP中，零次学习通常通过预训练的语言模型实现，如GPT-3，这些模型在巨大的文本语料库上预训练，学会了大量的语言知识和世界知识。因此，它们能够在没有特定任务训练数据的情况下，对新任务进行推理和生成回答。</p>
<p>问：解释卷积神经网络（CNN）在NLP中的应用。<br>答：尽管CNN主要用于图像处理，但它们也被用于NLP中的某些任务，特别是那些涉及到识别局部模式的任务，如文本分类和情感分析。在NLP中，CNN通过滑动窗口（卷积核）提取文本的n-gram特征，并通过池化层捕捉关键信号，这有助于模型理解文本的重要局部模式。</p>
<p>问：解释一下神经网络中的梯度消失问题及其对RNN的影响。<br>答：梯度消失问题是指在训练深层神经网络时，梯度随着传播到前层网络而逐渐变小，导致前层网络的权重更新非常缓慢，从而影响学习效率。在RNN中，由于依赖于长序列的时间依赖，这个问题尤为严重，因为梯度必须通过时间回传，长序列导致梯度极易消失。LSTM和GRU等RNN变体通过引入门控机制来缓解这一问题。</p>
<p>问：什么是图神经网络（GNN）？在NLP中的应用示例是什么？<br>答：图神经网络（GNN）是一种处理图结构数据的神经网络，它通过节点间的边来传递信息，更新节点状态。在NLP中，GNN可以用于诸如依存句法分析、文档分类等任务，其中文本可以被视为由单词（节点）和语义或句法关系（边）构成的图。</p>
<p>问：解释变分自编码器（VAE）在文本生成中的应用。<br>答：变分自编码器（VAE）是一种生成模型，它通过学习输入数据的潜在表示来生成新的数据。在文本生成中，VAE可以捕捉文本数据的深层次特征，并生成具有相似特征的新文本。它通过最小化重构误差和潜在表示的KL散度来训练，使模型能够生成多样化且连贯的文本。</p>
<p>问：解释自监督学习在NLP中的角色。<br>答：自监督学习是一种无需显式标注数据即可训练模型的方法。在NLP中，自监督学习通过构造任务，如预测文本中遮蔽词汇（BERT的MLM任务）或生成文本（如GPT系列），让模型学习语言的深层结构和语义。这种方法利用大量未标注文本数据，显著提高了模型的语言理解和生成能力。</p>
<p>问：什么是语言模型微调？其在下游NLP任务中的作用是什么？ 答：语言模型微调是指在预训练语言模型基础上，通过在特定下游任务的数据集上进行额外训练来优化模型性能的过程。这允许模型继承预训练中获得的广泛语言知识，并对特定任务进行优化，从而在诸如文本分类、情感分析、问答等任务中达到更好的性能。</p>
<p>问：什么是对比学习？在NLP中如何应用？<br>答：对比学习是一种学习方法，通过比较正样本（相似）与负样本（不相似）来学习表示的方法。在NLP中，对比学习可以用于改进词嵌入，通过让模型学习区分上下文中相似和不相似的词或短语，以获得更细致和区分度更高的词向量表示。</p>
<p>问：解释一下生成对抗网络（GAN）在NLP中的潜在应用。 答：生成对抗网络（GAN）是一种由生成器和判别器组成的框架，生成器生成数据，判别器评估数据的真实性。在NLP中，GAN可以用于文本生成任务，如生成逼真的文本、进行文本风格转换等。通过对抗训练，GAN能够产生高质量、多样化的文本数据。</p>
<p>问：解释交叉熵损失函数在分类任务中的作用及其数学表达式。 答：交叉熵损失函数是衡量模型预测概率分布与真实分布之间差异的一种方法，广泛用于分类任务。其数学表达式为：<br>其中 y_i 是真实分布的标签（通常为one-hot编码），p_i 是模型预测的概率。通过最小化交叉熵损失，模型学习提高对正确类别的预测概率。</p>
<p>问：什么是双向解码器，它在NLP中的应用是什么？ 答：双向解码器是指在生成任务中，同时考虑已生成序列的前文和后文来预测当前位置的模型结构。这种方法能够提高文本生成的一致性和相关性，尤其适用于摘要生成、文本修正等任务，其中上下文信息对于生成高质量文本至关重要。</p>
<p>问：解释Transformer模型中的多头注意力机制及其优势。 答：多头注意力机制是Transformer模型的一个关键组成部分，它允许模型在不同的表示子空间中并行地学习信息。通过将注意力分成多个“头”，模型可以在不同的位置捕捉到多样的依赖关系，从而提高模型的表达能力和注意力的覆盖范围。数学上，多头注意力通过将查询、键和值矩阵分割成多个头，然后并行计算，最后拼接结果，来实现这一点。</p>
<p>问：解释和推导基于梯度的学习算法中的反向传播算法。<br>答：反向传播算法用于计算神经网络中的梯度，以便通过梯度下降法更新权重。对于具有L层的网络，给定损失函数L，对于第l层的权重 ，梯度通过链式法则计算： 其中， 第l层的输出。反向传播算法首先计算损失函数相对于输出层激活的梯度，然后从输出层到输入层反向计算每层权重的梯度。这些梯度随后用于更新网络的权重，以最小化损失函数。</p>
<p>问：详细解释自然语言处理中使用的L2正则化的数学原理及其如何帮助防止模型过拟合。<br>答：L2正则化是一种减少模型过拟合的技术，通过在损失函数中添加一个正则化项，惩罚大的权重值。对于模型参数，加上L2正则化的损失函数表示为： 。L2正则化促使模型学习到小的权重值，从而减少模型的复杂度，提高泛化能力，防止过拟合。</p>
<p>问：解释并推导如何在神经网络中使用Dropout技术来防止过拟合，并给出其数学表达式。<br>答：Dropout是一种减少神经网络过拟合的技术，通过在训练过程中随机“丢弃”（即将输出设为0）一些神经元。这样，每次前向传播时，网络的一部分神经元不参与计算，减少了复杂的共适应关系，增强了模型的泛化能力。</p>
<p>问：在自注意力机制中，如何计算注意力权重，并解释它如何允许模型捕捉长距离依赖？<br>问：解释并推导Transformer模型中多头注意力的数学原理。<br>问：详细解释卷积神经网络（CNN）在处理文本时的数学原理，尤其是如何应用于文本分类任务。<br>问：详细解释如何使用梯度下降法优化NLP模型，包括数学公式。<br>问：详细解释在处理不平衡数据集时，如何使用加权交叉熵损失函数，并给出其数学表示。<br>问：解释注意力机制中“查询”、“键”、“值”的概念及其数学表示。</p>
<p>高频题：<br>问：简述NLP中的文本预处理步骤有哪些？ 答：文本预处理的常见步骤包括：标准化（如小写化）、分词、去除停用词、词干提取或词形还原、稀疏词去除、向量化等。这些步骤旨在将原始文本转换成适合机器学习模型处理的格式。</p>
<p>问：什么是词嵌入？它是如何工作的？ 答：词嵌入是将词语表示为密集向量的一种技术。每个词被映射到高维空间中的一个点，相似意义的词在该空间中的距离较近。词嵌入通常通过无监督学习算法（如Word2Vec、GloVe）在大型语料库上预训练得到。</p>
<p>问：解释一下什么是RNN，并讨论其在处理序列数据时面临的挑战。 答：循环神经网络（RNN）是一种处理序列数据的神经网络，具有维持一种内部状态的能力，该状态能够捕捉到目前为止处理的信息。RNN在处理长序列数据时面临梯度消失或梯度爆炸的挑战，这限制了其学习长期依赖的能力。</p>
<p>问：LSTM和GRU的主要区别是什么？ 答：LSTM（长短期记忆网络）和GRU（门控循环单元）都是为解决标准RNN长期依赖问题而设计的。它们的主要区别在于结构上：LSTM有三个门（输入门、遗忘门、输出门）和一个细胞状态，而GRU简化为两个门（更新门和重置门）且没有独立的细胞状态。</p>
<p>问：Transformer模型与RNN和CNN相比有什么优势？ 答：Transformer模型的主要优势在于其能够并行处理序列数据，极大提高了训练效率。它通过自注意力机制捕捉序列中的长距离依赖，无需逐步处理数据，解决了RNN处理长序列时的梯度消失问题。相比CNN，Transformer能更灵活地处理各种长度的依赖关系。</p>
<p>问：BERT模型是如何训练的？ 答：BERT模型通过两种预训练任务进行训练：遮蔽语言模型（MLM）和下一句预测（NSP）。在MLM中，模型学习预测随机遮蔽的词；在NSP中，模型学习预测两个句子是否顺序排列。这种预训练方式使BERT能够捕捉丰富的语言知识。</p>
<p>问：什么是注意力机制？它如何改进了序列模型的性能？ 答：注意力机制允许模型在生成每个输出时动态地“聚焦”于输入序列的最相关部分。这改进了序列模型的性能，尤其是在处理长序列和需要捕捉复杂依赖的任务中，因为它提供了一种灵活的方式来捕捉序列中的远程依赖。</p>
<p>问：为什么需要序列到序列模型？请给出一个应用场景。 答：序列到序列模型用于处理输入序列和输出序列长度不同的情况，它们在机器翻译、文本摘要等任务中非常有用。例如，在机器翻译中，模型需要将一种语言的文本转换成另一种语言的等价文本，输入和输出的长度通常不同。</p>
<p>问：解释一下自监督学习在NLP中的应用。 答：自监督学习是一种利用未标注数据进行预训练的方法，其中模型学习预测输入数据中的某些部分。在NLP中，这通常通过预测文本中遮蔽词汇的任务来实现，如BERT的预训练过程，使模型能够捕捉丰富的语言结构和语义信息。</p>
<p>请详细介绍 Transformer 模型的架构和原理。<br>Transformer 模型由 encoder 和 decoder 两个部分组成。encoder 负责将输入文本转换为一个向量序列，decoder 负责将向量序列转换为输出文本。encoder 由多个 self-attention 层组成，每个 self-attention 层都会学习句子中的全局依赖关系。decoder 由多个 self-attention 层和一个 attention 层组成，attention 层会学习输入文本和输出文本之间的依赖关系。<br>Transformer 模型的核心是注意力机制。注意力机制可以让模型学习句子中的全局依赖关系。具体来说，注意力机制会计算每个词与其他词之间的相关性，并根据相关性来调整每个词的权重。</p>
<p>请介绍你熟悉的大模型训练的并行化策略。<br>数据并行：将训练数据分割成多个子集，并在不同的机器上并行训练模型。<br>模型并行：将模型分割成多个子模型，并在不同的机器上并行训练子模型。<br>流水线并行：将训练过程分割成多个阶段，并在不同的机器上并行执行不同的阶段。</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/689965833">https://zhuanlan.zhihu.com/p/689965833</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/705789986">https://zhuanlan.zhihu.com/p/705789986</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/694373104">https://zhuanlan.zhihu.com/p/694373104</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643829565">https://zhuanlan.zhihu.com/p/643829565</a></p>
<h1 id="Stable-Diffusion"><a href="#Stable-Diffusion" class="headerlink" title="Stable Diffusion"></a>Stable Diffusion</h1><p>1、Stable Diffusion里是如何用文本来控制生成的，Unet的Attention模块具体是如何实现的<br>Stable Diffusion是一种潜在扩散模型，主要通过自动编码器（VAE），U-Net以及文本编码器三个核心组件完成用文本来控制生成的图像。Unet的Attention模块Latent Feature和Context Embedding作为输入，将两者进行Cross Attenetion操作，将图像信息和文本信息进行了融合，整体上是一个经典的Transformer流程。</p>
<p>Cross Attention与softmax<br>Cross Attention：在Transformer模型中，Cross Attention通常用于“编码器-解码器”结构中的解码器，允许解码器关注编码器的输出。解码器可以使用这种注意力权重来决定从编码器输出中关注哪些部分。</p>
<p>Softmax：Softmax函数是一种将向量转化为概率分布的函数。给定一个实数向量，它会输出一个相同维度的新向量，其中每个值都是在0和1之间，并且所有值的总和为1。具体地说，softmax函数的第i个元素的值是原向量的第i个元素的指数与所有元素的指数之和的比值。 </p>
<p>Focal Loss<br>Focal Loss是为了解决目标检测任务中正负样本不均衡问题而提出的。在传统的交叉熵损失中，容易出现大量的负样本主导损失，导致模型无法学习到稀有或困难的正样本。Focal Loss通过给予那些容易分类的样本更小的权重来减少这种影响。</p>
<p>作者：七月在线-julyedu <a target="_blank" rel="noopener" href="https://www.bilibili.com/read/cv26297685/">https://www.bilibili.com/read/cv26297685/</a> 出处：bilibili</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2024/07/28/20240728_Python%E8%AF%AD%E8%A8%80%E7%9B%B8%E5%85%B3/" rel="next" title="Python语言相关">
                <i class="fa fa-chevron-left"></i> Python语言相关
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2024/11/23/20241123_%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1/" rel="prev" title="开发面试八股">
                开发面试八股 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/custom/jiuzhou.jpg"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Stable-Diffusion"><span class="nav-number">1.</span> <span class="nav-text">Stable Diffusion</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JiuZhouu</span>

  
</div>






  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
